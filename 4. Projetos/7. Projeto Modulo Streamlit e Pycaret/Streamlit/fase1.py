import streamlit as st
import pandas as pd
import numpy as np
import seaborn as sns

from matplotlib import pyplot as plt
from scipy.stats import t
from scipy.stats import ks_2samp

import statsmodels.formula.api as smf
import statsmodels.api as sm
from sklearn import metrics
########################################################################
# C√≥digo 
df = pd.read_feather('credit_scoring.ftr')
oot = df[pd.to_datetime(df.data_ref) > pd.to_datetime(df.data_ref.max()) - pd.DateOffset(months=3)]
train = df[pd.to_datetime(df.data_ref) <= pd.to_datetime(df.data_ref.max()) - pd.DateOffset(months=3)]
train_counts = train["data_ref"].value_counts().sort_index()
oot_counts = oot["data_ref"].value_counts().sort_index()

df["tempo_emprego"] = df["tempo_emprego"].fillna(df.groupby(["tipo_renda"])["tempo_emprego"].transform("mean"))

train = df[pd.to_datetime(df.data_ref) <= pd.to_datetime(df.data_ref.max()) - pd.DateOffset(months=3)]
df_ = train.drop(["index", "data_ref"], axis = 1)
des_uni = pd.DataFrame({
    "variavel": df_.columns,
    "dtype": df_.dtypes.astype(str),
    "nmissing": df_.isna().sum().values,
    "valores_unicos": df_.nunique().values
})

fig1, ax = plt.subplots(figsize=(6, 4))
sns.barplot(x=train_counts.index.strftime('%m-%Y'), y=train_counts.values, color="violet", ax=ax)
ax.set_xlabel("M√™s de refer√™ncia")
ax.set_ylabel("Quantidade de registros")
plt.xticks(rotation=45)

fig2, ax = plt.subplots(figsize=(6, 4))
sns.barplot(x=oot_counts.index.strftime('%m-%Y'), y=oot_counts.values, color="pink", ax=ax)
ax.set_xlabel("M√™s de refer√™ncia")
ax.set_ylabel("Quantidade de registros")

df_ = train.drop(["index", "data_ref"], axis = 1)

des_uni = pd.DataFrame({
    "variavel": df_.columns,
    "dtype": df_.dtypes.astype(str),
    "nmissing": df_.isna().sum().values,
    "valores_unicos": df_.nunique().values
})

des_uni["tipo_var"] = des_uni["dtype"].apply(lambda x: "quantitativo" if any(t in x for t in ["float", "int"]) else "qualitativo")
des_uni["papel"] = des_uni["variavel"].apply(lambda x: "resposta" if x == "mau" else "covariavel")
des_uni["moda"] = des_uni["variavel"].apply(lambda v: df[v].mode().iloc[0] if not df[v].mode().empty else None)
des_uni["menos_comum"] = des_uni["variavel"].apply(lambda v: df[v].value_counts().idxmin() if df[v].nunique() > 0 else None)
des_uni["m√≠nimo"] = des_uni["variavel"].apply(lambda v: round(df[v].min(), 2) if des_uni.loc[des_uni["variavel"] == v, "tipo_var"].item() == "quantitativo" else "-")
des_uni["m√°ximo"] = des_uni["variavel"].apply(lambda v: round(df[v].max(), 2) if des_uni.loc[des_uni["variavel"] == v, "tipo_var"].item() == "quantitativo" else "-")
des_uni["m√©dia"] = des_uni["variavel"].apply(lambda v: round(df[v].mean(), 2) if des_uni.loc[des_uni["variavel"] == v, "tipo_var"].item() == "quantitativo" else "-")
des_uni = des_uni.drop(["variavel"], axis = 1)

def IV(variavel, resposta):
    tab = pd.crosstab(variavel, resposta, margins=True, margins_name='total')

    r√≥tulo_evento = tab.columns[0]
    r√≥tulo_nao_evento = tab.columns[1]

    tab['pct_evento'] = tab[r√≥tulo_evento]/tab.loc['total',r√≥tulo_evento]
    tab['ep'] = tab[r√≥tulo_evento]/tab.loc['total',r√≥tulo_evento]
    
    tab['pct_nao_evento'] = tab[r√≥tulo_nao_evento]/tab.loc['total',r√≥tulo_nao_evento]
    tab['woe'] = np.log(tab.pct_evento/tab.pct_nao_evento)
    tab['iv_parcial'] = (tab.pct_evento - tab.pct_nao_evento)*tab.woe
    return tab['woe'].sum(), tab['iv_parcial'].sum()

for var in des_uni[des_uni.papel=="covariavel"].index:
    if  (des_uni.loc[var, "valores_unicos"]>15):
       woe, iv_total = IV(pd.qcut(df[var],q = 10,duplicates='drop'), df.mau)
    else: 
       woe, iv_total  = IV(df[var], df["mau"])
        
    des_uni.loc[var, "woe"] = woe
    des_uni.loc[var, "IV"] = iv_total

des_uni.loc["bom", "dtype"] = "bool"
des_uni.loc["bom", "tipo_var"] = "qualitativo"
des_uni.loc["bom", "papel"] = "resposta"

formula = '''
    mau ~ I(idade**2) + I(-tempo_emprego) + I(1/renda) 
'''

rl = smf.glm(formula, data=train, family=sm.families.Binomial()).fit()

oot = df[pd.to_datetime(df.data_ref) > pd.to_datetime(df.data_ref.max()) - pd.DateOffset(months=3)]

oot['score'] = rl.predict(oot)

# Acur√°cia
acc = metrics.accuracy_score(oot.mau, oot.score>.068)
#AUC
fpr, tpr, thresholds = metrics.roc_curve(oot.mau, oot.score)
auc = metrics.auc(fpr, tpr)
#Gini
gini = 2*auc -1
ks = ks_2samp(oot.loc[oot.mau == 1, 'score'], oot.loc[oot.mau != 1, 'score']).statistic


########################################################################

st.title("üêç Estudo dos Dados")

st.markdown(''' ## Amostragem
Separe os tr√™s √∫ltimos meses como safras de valida√ß√£o out of time (oot).

Vari√°veis:
Considere que a vari√°vel data_ref n√£o √© uma vari√°vel explicativa, √© somente uma vari√°vel indicadora da safra, e n√£o deve ser utilizada na modelagem. A vari√°vei index √© um identificador do cliente, e tamb√©m n√£o deve ser utilizada como covari√°vel (vari√°vel explicativa). As restantes podem ser utilizadas para prever a inadimpl√™ncia, incluindo a renda.

Para separa√ß√£o da amostragem o c√≥digo que se segue foi utilizado
''')

st.code('''
oot = df[pd.to_datetime(df.data_ref) > pd.to_datetime(df.data_ref.max()) - pd.DateOffset(months=3)]
train = df[pd.to_datetime(df.data_ref) <= pd.to_datetime(df.data_ref.max()) - pd.DateOffset(months=3)]
''', language = "python")

st.write("Datas presentes na base de desenvolvimento do modelo: ", train["data_ref"].unique())
st.write("Datas presentes na base de valida√ß√£o (out of time) do modelo: ", oot["data_ref"].unique())

st.markdown('''---
## An√°lise Descritiva B√°sica Univariada
* Descreva a base quanto ao n√∫mero de linhas, n√∫mero de linhas para cada m√™s em data_ref.
* Fa√ßa uma descritiva b√°sica univariada de cada vari√°vel. Considere as naturezas diferentes: qualitativas e quantitativas.

N√∫mero de An√°lise por M√™s - Distribui√ß√£o de registros por m√™s ‚Äî Base de estudo
''')
st.pyplot(fig1)

st.markdown('''N√∫mero de An√°lise por M√™s - Distribui√ß√£o de registros por m√™s ‚Äî Base Out of Time (OOT)
''')
st.pyplot(fig2)

st.markdown('''Para desenvolvimento da an√°lise descritiva de cada vari√°vel utilizei o seguinte c√≥digo que gerou o seguinte recultado em tabela
''')

st.code('''df_ = train.drop(["index", "data_ref"], axis = 1)

des_uni = pd.DataFrame({
    "variavel": df_.columns,
    "dtype": df_.dtypes.astype(str),
    "nmissing": df_.isna().sum().values,
    "valores_unicos": df_.nunique().values
})

des_uni["tipo_var"] = des_uni["dtype"].apply(lambda x: "quantitativo" if any(t in x for t in ["float", "int"]) else "qualitativo")
des_uni["papel"] = des_uni["variavel"].apply(lambda x: "resposta" if x == "mau" else "covariavel")
des_uni["moda"] = des_uni["variavel"].apply(lambda v: df[v].mode().iloc[0] if not df[v].mode().empty else None)
des_uni["menos_comum"] = des_uni["variavel"].apply(lambda v: df[v].value_counts().idxmin() if df[v].nunique() > 0 else None)
des_uni["m√≠nimo"] = des_uni["variavel"].apply(lambda v: round(df[v].min(), 2) if des_uni.loc[des_uni["variavel"] == v, "tipo_var"].item() == "quantitativo" else "-")
des_uni["m√°ximo"] = des_uni["variavel"].apply(lambda v: round(df[v].max(), 2) if des_uni.loc[des_uni["variavel"] == v, "tipo_var"].item() == "quantitativo" else "-")
des_uni["m√©dia"] = des_uni["variavel"].apply(lambda v: round(df[v].mean(), 2) if des_uni.loc[des_uni["variavel"] == v, "tipo_var"].item() == "quantitativo" else "-")
des_uni = des_uni.drop(["variavel"], axis = 1)
''', language = "python")

st.write(des_uni)


st.write('''A primeira observa√ß√£o notada √© que h√° missing valores apenas entre os dados de tempo_emprego. Notamos que h√° uma grande variabilidade entre os dados coletados de tempo_emprego e renda. E uma grande quantidade, se destacando em rela√ß√£o √†s outras, de respostas √∫nicas para essas mesmas vari√°veis.''')
st.markdown('''---
## An√°lise Descritiva Bivariada

Para essa an√°lise utilizei o seguinte c√≥digo, com o resultado apresentado:
''')
st.code('''def IV(variavel, resposta):
    tab = pd.crosstab(variavel, resposta, margins=True, margins_name='total')

    r√≥tulo_evento = tab.columns[0]
    r√≥tulo_nao_evento = tab.columns[1]

    tab['pct_evento'] = tab[r√≥tulo_evento]/tab.loc['total',r√≥tulo_evento]
    tab['ep'] = tab[r√≥tulo_evento]/tab.loc['total',r√≥tulo_evento]
    
    tab['pct_nao_evento'] = tab[r√≥tulo_nao_evento]/tab.loc['total',r√≥tulo_nao_evento]
    tab['woe'] = np.log(tab.pct_evento/tab.pct_nao_evento)
    tab['iv_parcial'] = (tab.pct_evento - tab.pct_nao_evento)*tab.woe
    return tab['woe'].sum(), tab['iv_parcial'].sum()

for var in des_uni[des_uni.papel=="covariavel"].index:
    if  (des_uni.loc[var, "valores_unicos"]>15):
       woe, iv_total = IV(pd.qcut(df[var],q = 10,duplicates='drop'), df.mau)
    else: 
       woe, iv_total  = IV(df[var], df["mau"])
        
    des_uni.loc[var, "woe"] = woe
    des_uni.loc[var, "IV"] = iv_total

des_uni.loc["bom", "dtype"] = "bool"
des_uni.loc["bom", "tipo_var"] = "qualitativo"
des_uni.loc["bom", "papel"] = "resposta"
''', language = "python")

st.write(des_uni)

st.markdown('''Sabemos que vari√°veis com IV inferior a 0,02 possuem poder preditivo praticamente nulo. Portanto, elas ser√£o desconsideradas nas pr√≥ximas etapas da an√°lise. S√£o elas:

* sexo
* posse_de_veiculo
* posse_de_imovel
* qtd_filhos
* tipo_renda
* educacao
* estad_civil
* tipo_residencia

Al√©m disso, observa-se que a vari√°vel qt_pessoas_residencia apresentou IV = inf, o que indica separa√ß√£o perfeita entre as classes ‚Äî ou seja, h√° categorias que ocorrem exclusivamente em um dos grupos (‚Äúbom‚Äù ou ‚Äúmau‚Äù). Esse comportamento pode sinalizar problemas de representatividade ou inconsist√™ncia nos dados, e por isso a vari√°vel deve ser reavaliada, possivelmente por meio de reagrupamento de categorias ou tratamento de outliers.

Observa-se, tamb√©m, que a vari√°vel renda apresenta um IV muito elevado, o que pode indicar risco de sobreajuste (overfitting) ou forte correla√ß√£o com a vari√°vel resposta.

Contudo, considerando que o estudo tem como objetivo analisar a inadimpl√™ncia de indiv√≠duos, faz sentido manter a vari√°vel para avalia√ß√£o, visto que ela √© conceitualmente relevante para o contexto da an√°lise.
''')

des_uni = des_uni.loc[des_uni.index.isin(["idade", "tempo_emprego", "renda", "qt_pessoas_residencia", "mau", "bom"])]
st.write(des_uni)

#########################################################################################################
st.markdown('''
### An√°lise Gr√°fica dos Dados
Pra an√°lise gr√°fica de distribui√ß√£o dos dados estudados utilizarei os seguintes c√≥digos
Para vari√°veis discretas:''') 
st.code('''def biv_discreta(var, df):
    df['bom'] = 1 - df['mau']

    g = df.groupby(var)
    biv = pd.DataFrame({
        'qt_bom': g['bom'].sum(),
        'qt_mau': g['mau'].sum(),
        'mau': g['mau'].mean(),
        'cont': g[var].count()
    }).reset_index()

    # Intervalos de confian√ßa
    biv['ep'] = (biv['mau'] * (1 - biv['mau']) / biv['cont']) ** 0.5
    biv['mau_sup'] = biv['mau'] + t.ppf(0.975, biv['cont'] - 1) * biv['ep']
    biv['mau_inf'] = biv['mau'] - t.ppf(0.975, biv['cont'] - 1) * biv['ep']

    tx_mau_geral = df['mau'].mean()
    woe_geral = np.log(tx_mau_geral / (1 - tx_mau_geral))

    biv['logit'] = np.log(biv['mau'] / (1 - biv['mau']))
    biv['woe'] = biv['logit'] - woe_geral

    biv['iv_parcial'] = ((biv['qt_mau'] / df['mau'].sum()) -
                         (biv['qt_bom'] / df['bom'].sum())) * biv['woe']
    biv['IV_total'] = biv['iv_parcial'].sum()

    print(f"\nüîπ Vari√°vel: {var}")
    print(f"üìä IV total: {biv['IV_total'].iloc[0]:.5f}\n")
    print(biv[[var, 'qt_mau', 'qt_bom', 'mau', 'woe', 'iv_parcial']].to_string(index=False))

    fig, ax = plt.subplots(2, 1, figsize=(10, 6))
    ax[0].plot(biv[var], biv['woe'], ':bo', label='WOE')
    ax[0].set_ylabel("Weight of Evidence")
    ax[0].set_title(f"WOE de {var}")
    ax[0].legend()

    biv['cont'].plot.bar(ax=ax[1])
    ax[1].set_title("Frequ√™ncia por categoria")

    plt.tight_layout()
    plt.show()

    return biv
''', language = "python")

#########################################################################################################
st.write("Para vari√°veis cont√≠nuas:")
st.code('''def biv_continua(var, df, ncat=None, bins=None, labels=None):
    df_local = df.copy()
    df_local['bom'] = 1 - df_local['mau']

    # Criar categoria
    if bins is not None:
        df_local['categoria'] = pd.cut(df_local[var], bins=bins, labels=labels, include_lowest=True)
    elif ncat is not None:
        df_local['categoria'], bins = pd.qcut(df_local[var], ncat, retbins=True, precision=0, duplicates='drop')
    else:
        raise ValueError("Voc√™ deve fornecer ncat ou bins.")

    g = df_local.groupby('categoria')

    biv = pd.DataFrame({
        'qt_bom': g['bom'].sum(),
        'qt_mau': g['mau'].sum(),
        'mau': g['mau'].mean(), 
        var: g[var].mean(), 
        'cont': g[var].count()
    })

    # Intervalos de confian√ßa para mau
    biv['ep'] = (biv['mau'] * (1 - biv['mau']) / biv['cont']) ** 0.5
    biv['mau_sup'] = biv['mau'] + t.ppf(0.975, biv['cont'] - 1) * biv['ep']
    biv['mau_inf'] = biv['mau'] - t.ppf(0.975, biv['cont'] - 1) * biv['ep']

    tx_mau_geral = df_local['mau'].mean()
    woe_geral = np.log(tx_mau_geral / (1 - tx_mau_geral))

    # WOE e IV
    biv['logit'] = np.log(biv['mau'] / (1 - biv['mau']))
    biv['woe'] = biv['logit'] - woe_geral
    biv['iv_parcial'] = ((biv['qt_mau'] / df_local['mau'].sum()) -
                         (biv['qt_bom'] / df_local['bom'].sum())) * biv['woe']
    biv['IV_total'] = biv['iv_parcial'].sum()

    print(f"\nüîπ Vari√°vel: {var}")
    print(f"üìä IV total: {biv['IV_total'].iloc[0]:.5f}\n")
    print(biv[[var, 'qt_mau', 'qt_bom', 'mau', 'woe', 'iv_parcial']].to_string(index=False))

    # Gr√°ficos
    fig, ax = plt.subplots(2, 1, figsize=(10, 6))
    ax[0].plot(biv[var], biv['woe'], ':bo', label='WOE')
    ax[0].set_ylabel("Weight of Evidence")
    ax[0].set_title(f"WOE de {var}")
    ax[0].legend()
    biv['cont'].plot.bar(ax=ax[1])
    ax[1].set_title("Frequ√™ncia por categoria")
    plt.tight_layout()
    plt.show()

    return biv
''', language = "python")
#########################################################################################################

def biv_continua(var, df, ncat=None, bins=None, labels=None):
    df_local = df.copy()
    df_local['bom'] = 1 - df_local['mau']

    # Criar categoria
    if bins is not None:
        df_local['categoria'] = pd.cut(df_local[var], bins=bins, labels=labels, include_lowest=True)
    elif ncat is not None:
        df_local['categoria'], bins = pd.qcut(df_local[var], ncat, retbins=True, precision=0, duplicates='drop')
    else:
        raise ValueError("Voc√™ deve fornecer ncat ou bins.")

    g = df_local.groupby('categoria')

    biv = pd.DataFrame({
        'qt_bom': g['bom'].sum(),
        'qt_mau': g['mau'].sum(),
        'mau': g['mau'].mean(), 
        var: g[var].mean(), 
        'cont': g[var].count()
    })

    # Intervalos de confian√ßa para mau
    biv['ep'] = (biv['mau'] * (1 - biv['mau']) / biv['cont']) ** 0.5
    biv['mau_sup'] = biv['mau'] + t.ppf(0.975, biv['cont'] - 1) * biv['ep']
    biv['mau_inf'] = biv['mau'] - t.ppf(0.975, biv['cont'] - 1) * biv['ep']

    tx_mau_geral = df_local['mau'].mean()
    woe_geral = np.log(tx_mau_geral / (1 - tx_mau_geral))

    # WOE e IV
    biv['logit'] = np.log(biv['mau'] / (1 - biv['mau']))
    biv['woe'] = biv['logit'] - woe_geral
    biv['iv_parcial'] = ((biv['qt_mau'] / df_local['mau'].sum()) -
                         (biv['qt_bom'] / df_local['bom'].sum())) * biv['woe']
    biv['IV_total'] = biv['iv_parcial'].sum()

    st.write(f"\nüîπ Vari√°vel: {var}")
    st.write(f"üìä IV total: {biv['IV_total'].iloc[0]:.5f}\n")
    st.table(biv[[var, 'qt_mau', 'qt_bom', 'mau', 'woe', 'iv_parcial']].reset_index(drop=True))

    # Gr√°ficos
    fig, ax = plt.subplots(2, 1, figsize=(10, 6))
    ax[0].plot(biv[var], biv['woe'], ':bo', label='WOE', color = "violet")
    ax[0].set_ylabel("Weight of Evidence")
    ax[0].set_title(f"WOE de {var}")
    ax[0].legend()
    biv['cont'].plot.bar(ax=ax[1], color = "pink")
    ax[1].set_title("Frequ√™ncia por categoria")
    plt.tight_layout()
    st.pyplot(fig)
    
    st.write("-"*100)

    return biv

#########################################################################################################

def biv_discreta(var, df):
    df['bom'] = 1 - df['mau']

    g = df.groupby(var)
    biv = pd.DataFrame({
        'qt_bom': g['bom'].sum(),
        'qt_mau': g['mau'].sum(),
        'mau': g['mau'].mean(),
        'cont': g[var].count()
    }).reset_index()

    # Intervalos de confian√ßa
    biv['ep'] = (biv['mau'] * (1 - biv['mau']) / biv['cont']) ** 0.5
    biv['mau_sup'] = biv['mau'] + t.ppf(0.975, biv['cont'] - 1) * biv['ep']
    biv['mau_inf'] = biv['mau'] - t.ppf(0.975, biv['cont'] - 1) * biv['ep']

    tx_mau_geral = df['mau'].mean()
    woe_geral = np.log(tx_mau_geral / (1 - tx_mau_geral))

    biv['logit'] = np.log(biv['mau'] / (1 - biv['mau']))
    biv['woe'] = biv['logit'] - woe_geral

    biv['iv_parcial'] = ((biv['qt_mau'] / df['mau'].sum()) -
                         (biv['qt_bom'] / df['bom'].sum())) * biv['woe']
    biv['IV_total'] = biv['iv_parcial'].sum()

    print(f"\nüîπ Vari√°vel: {var}")
    print(f"üìä IV total: {biv['IV_total'].iloc[0]:.5f}\n")
    print(biv[[var, 'qt_mau', 'qt_bom', 'mau', 'woe', 'iv_parcial']].to_string(index=False))

    fig, ax = plt.subplots(2, 1, figsize=(10, 6))
    ax[0].plot(biv[var], biv['woe'], ':bo', label='WOE')
    ax[0].set_ylabel("Weight of Evidence")
    ax[0].set_title(f"WOE de {var}")
    ax[0].legend()

    biv['cont'].plot.bar(ax=ax[1])
    ax[1].set_title("Frequ√™ncia por categoria")

    plt.tight_layout()
    plt.show()

    return biv

#########################################################################################################
var = des_uni.index.unique()
var_sel = st.multiselect("Selecione a vari√°vel:", var, default=var)

# Filtrar DataFrame de acordo com sele√ß√£o
df_filtrado = des_uni.loc[var_sel]
st.dataframe(df_filtrado)

# Separar vari√°veis qualitativas e quantitativas
qual_vars = df_filtrado[(df_filtrado['papel'] == "covariavel") & (df_filtrado['tipo_var'] == "qualitativo")].index
quant_vars = df_filtrado[(df_filtrado['papel'] == "covariavel") & (df_filtrado['tipo_var'] == "quantitativo")].index

# Processar vari√°veis qualitativas
for var_q in qual_vars:
    biv = biv_discreta(var_q, train) 

# Processar vari√°veis quantitativas
for var_qt in quant_vars:
    valores_unicos = des_uni.loc[var_qt, "valores_unicos"]
    
    if 16 < valores_unicos < 10001:
        gr = 15
    elif valores_unicos > 10000:
        gr = 20
    else:
        gr = valores_unicos

    gr = max(int(gr), 1)  # Garantir valor m√≠nimo de 1
    biv_continua(var_qt, train, ncat=gr)  

##########################################################################################################
st.markdown('''
## Desenvolvimento do Modelo

Desenvolva um modelo de credit scoring atrav√©s de uma regress√£o log√≠stica.
* Trate valores missings e outliers
* Trate 'zeros estruturais'
* Fa√ßa agrupamentos de categorias conforme vimos em aula
* Proponha uma equa√ß√£o preditiva para 'mau'
* Caso hajam categorias n√£o significantes, justifique

Para tratamento dos dados faltantes de tempo_emprego substituirei os valores pela m√©dia por tipo de renda, utilizando o c√≥digo:''')

st.code('''df["tempo_emprego"] = df["tempo_emprego"].fillna(df.groupby(["tipo_renda"])["tempo_emprego"].transform("mean"))
''', language = "python")

st.write("Tamb√©m podemos notra ap√≥s observa√ß√£o da etapa anterior a exclus√£o de vari√°veis de baixa relev√¢ncia realizada na etapa anterior ‚Äî baseada na an√°lise de Information Value (IV), uma m√©trica chave para identificar o poder preditivo de cada feature em rela√ß√£o √† inadimpl√™ncia ‚Äî, n√£o restaram dados discretos (categ√≥ricos) na base de dados.")

############################################################################################################
st.write("Para melhoria da an√°lise e tratamento de outliers proponho o reagrupamento observado na an√°lise biavariada dos dados como:")

st.write("Novos grupos de idade:")

st.code(''' 
idades = [
    "26-29.5",
    "30-35",
    "36-40",
    "41-46",
    "47-52",
    "53-54",
    "55-62",
    "64-65"
]
''', language = "python")

st.write("Novos grupos de tempo_emprego:")

st.code(''' 
tempo_emprego = [
    "0-1.9",
    "2-5.8",
    "6-7",
    "7.1-9.5",
    "11-25"
]
''', language = "python")

st.write("Novos grupos de renda:")

st.code(''' 
renda = [
    "0-1190", 
    "1190-1753", 
    "1753-2297", 
    "2297-2861", 
    "2861-3468", 
    "3468-4141", 
    "4141-4892", 
    "4892-5747", 
    "5747-6728",
    "6728-7862",
    "7862-9186",
    "9186-10793",
    "10793-18318",
    "18318-29748",
    "29748-4083986"
]
''', language = "python")

############################################################################################################
def biv_continua(var, df, ncat=None, bins=None, labels=None):
    df_local = df.copy()
    df_local['bom'] = 1 - df_local['mau']

    # Criar categoria
    if bins is not None:
        df_local['categoria'] = pd.cut(df_local[var], bins=bins, labels=labels, include_lowest=True)
    elif ncat is not None:
        df_local['categoria'], bins = pd.qcut(df_local[var], ncat, retbins=True, precision=0, duplicates='drop')
    else:
        raise ValueError("Voc√™ deve fornecer ncat ou bins.")

    g = df_local.groupby('categoria')

    biv = pd.DataFrame({
        'qt_bom': g['bom'].sum(),
        'qt_mau': g['mau'].sum(),
        'mau': g['mau'].mean(), 
        var: g[var].mean(), 
        'cont': g[var].count()
    })

    # Intervalos de confian√ßa para mau
    biv['ep'] = (biv['mau'] * (1 - biv['mau']) / biv['cont']) ** 0.5
    biv['mau_sup'] = biv['mau'] + t.ppf(0.975, biv['cont'] - 1) * biv['ep']
    biv['mau_inf'] = biv['mau'] - t.ppf(0.975, biv['cont'] - 1) * biv['ep']

    tx_mau_geral = df_local['mau'].mean()
    woe_geral = np.log(tx_mau_geral / (1 - tx_mau_geral))

    # WOE e IV
    biv['logit'] = np.log(biv['mau'] / (1 - biv['mau']))
    biv['woe'] = biv['logit'] - woe_geral
    biv['iv_parcial'] = ((biv['qt_mau'] / df_local['mau'].sum()) -
                         (biv['qt_bom'] / df_local['bom'].sum())) * biv['woe']
    biv['IV_total'] = biv['iv_parcial'].sum()

    st.write(f"\nüîπ Vari√°vel: {var}")
    st.write(f"üìä IV total: {biv['IV_total'].iloc[0]:.5f}\n")
    st.table(biv[[var, 'qt_mau', 'qt_bom', 'mau', 'woe', 'iv_parcial']].reset_index(drop=True))

    # Gr√°ficos
    fig, ax = plt.subplots(2, 1, figsize=(10, 6))
    ax[0].plot(biv[var], biv['woe'], ':bo', label='WOE', color = "violet")
    ax[0].set_ylabel("Weight of Evidence")
    ax[0].set_title(f"WOE de {var}")
    ax[0].legend()
    biv['cont'].plot.bar(ax=ax[1], color = "pink")
    ax[1].set_title("Frequ√™ncia por categoria")
    plt.tight_layout()
    st.pyplot(fig)
    
    st.write("-"*100)

    return biv

############################################################################################################

bins = [26, 29.5, 35, 40, 46, 52, 54, 62, 65]
labels = [
    "26-29.5",
    "30-35",
    "36-40",
    "41-46",
    "47-52",
    "53-54",
    "55-62",
    "64-65"
]

biv_continua("idade", df = df_, bins = bins, labels = labels)


bins_te = [0, 1.9, 5.8, 7.0, 9.5, 25]
labels_te = [
    "0-1.9",
    "2-5.8",
    "6-7",
    "7.1-9.5",
    "11-25"
]
biv_continua("tempo_emprego", df = df_, bins = bins_te, labels = labels_te)


bins_renda = [
    0,     
    1190,
    1753, 
    2297,
    2861, 
    3468, 
    4141, 
    4892, 
    5747, 
    6728, 
    7862, 
    9186, 
    10793,    
    18318,    
    29748,    
    4083986   
]

labels_renda = [
    "0-1190", 
    "1190-1753", 
    "1753-2297", 
    "2297-2861", 
    "2861-3468", 
    "3468-4141", 
    "4141-4892", 
    "4892-5747", 
    "5747-6728",
    "6728-7862",
    "7862-9186",
    "9186-10793",
    "10793-18318",
    "18318-29748",
    "29748-4083986"
]


biv_continua("renda", df = df_, bins = bins_renda, labels = labels_renda)

############################################################################################################
st.write("Equa√ß√£o preditiva proposta")
st.write("Primeiro analisarei uma f√≥rmula simples")
st.code('''
formula = 
    mau ~ idade + tempo_emprego  + renda + qt_pessoas_residencia


rl = smf.glm(formula, data=df_, family=sm.families.Binomial()).fit()
''', language = "python")

formula = '''
    mau ~ idade + tempo_emprego  + renda + qt_pessoas_residencia
'''

rl = smf.glm(formula, data=train, family=sm.families.Binomial()).fit()

st.write(rl.summary())
############################################################################################################
st.write("Pelos gr√°ficos observados anteriormente percebmos que idade tem uma forma semalhante a uma equa√ß√£o concava de 2¬∫ grau, tempo_emprego uma equa√ß√£o inversa linear, qt_emprego_residencia convexa de 2¬∫ grau e em rela√ß√£o a renda uma hib√©rbole positiva")
st.code('''
formula = 
    mau ~ I(idade**2) + tempo_emprego + I(1/renda) + I(qt_pessoas_residencia**2)
''', language = "python")

formula = '''
    mau ~ I(idade**2) + tempo_emprego + I(1/renda) + I(qt_pessoas_residencia**2)
'''

rl = smf.glm(formula, data=train, family=sm.families.Binomial()).fit()

st.write(rl.summary())
############################################################################################################
st.write("Considerando os valores dos primeiros modelos excluirei da f√≥rmula qt_pessoas_residencia por ter valor de p muito alto em ambas os resultados")
st.code('''
formula = 
    mau ~ I(idade**2) + I(-tempo_emprego) + I(1/renda) 
''', language = "python")

formula = '''
    mau ~ I(idade**2) + I(-tempo_emprego) + I(1/renda) 
'''

rl = smf.glm(formula, data=train, family=sm.families.Binomial()).fit()

st.write(rl.summary())
############################################################################################################

st.write("Foram testados tr√äs modelos de regress√£o log√≠stica com o objetivo de avaliar o impacto das vari√°veis idade, tempo de emprego, renda e quantidade de pessoas na resid√™ncia sobre a probabilidade de inadimpl√™ncia. O primeiro modelo considerou as vari√°veis em sua forma original, enquanto o segundo aplicou transforma√ß√µes n√£o lineares (idade¬≤, 1/renda e qt_pessoas_residencia¬≤), visando capturar rela√ß√µes mais complexas entre os preditores e a vari√°vel resposta.")

st.write("Os segundo modelo apresentou melhor ajuste estat√≠stico, com redu√ß√£o na deviance (de 2.4985e+05 para 2.4951e+05) e aumento do Pseudo R¬≤ de 0.04246 para 0.04301, indicando leve melhora na explica√ß√£o da vari√¢ncia da resposta. Al√©m disso, as vari√°veis transformadas (idade¬≤, tempo_emprego e 1/renda) mostraram-se altamente significativas (p < 0.001), sugerindo que essas rela√ß√µes n√£o lineares descrevem melhor o comportamento dos dados.")

st.write("Por outro lado, a vari√°vel qt_pessoas_residencia, mesmo ap√≥s a transforma√ß√£o quadr√°tica, n√£o apresentou signific√¢ncia estat√≠stica (p = 0.162) e seu impacto sobre o ajuste do modelo foi nulo. A exclus√£o dessa vari√°vel resultou em um modelo final mais parcimonioso, mantendo o mesmo n√≠vel de ajuste (Pseudo R¬≤ = 0.04301) e a mesma log-verossimilhan√ßa, mas com menor complexidade.")

st.write("Em s√≠ntese, o modelo final ‚Äî composto por idade¬≤, tempo de emprego e renda (1/renda) ‚Äî mostrou-se estatisticamente robusto, coerente com a teoria e mais eficiente, representando a melhor alternativa entre os testados. A rela√ß√£o entre as vari√°veis e a inadimpl√™ncia manteve-se economicamente plaus√≠vel: quanto maior a renda e o tempo de emprego, menor a probabilidade de inadimpl√™ncia, enquanto a idade apresentou uma rela√ß√£o n√£o linear, sugerindo que o risco √© maior em faixas et√°rias intermedi√°rias.")


#############################################
st.markdown('''
## Avalia√ß√£o do Modelo
Avaliando o poder discriminante do modelo pelo menos avaliando acur√°cia, KS e Gini. Utiizando essas m√©tricas nas bases de desenvolvimento e out of time. An√°lise realizado com o c√≥digo:''')

st.code('''oot['score'] = rl.predict(oot)

# Acur√°cia
acc = metrics.accuracy_score(oot.mau, oot.score>.068)
#AUC
fpr, tpr, thresholds = metrics.roc_curve(oot.mau, oot.score)
auc = metrics.auc(fpr, tpr)
#Gini
gini = 2*auc -1
ks = ks_2samp(oot.loc[oot.mau == 1, 'score'], oot.loc[oot.mau != 1, 'score']).statistic

print('Acur√°cia: {0:.1%} \nAUC: {1:.1%} \nGINI: {2:.1%}\nKS: {3:.1%}'
      .format(acc, auc, gini, ks))
''', language = "python")

st.write("Acur√°cia: {0:.1%}".format(acc)) 
st.write("AUC: {0:.1%}".format(auc))
st.write("GINI: {0:.1%}".format(gini))
st.write("KS: {0:.1%}".format(ks))


st.write("Apesar da consist√™ncia estat√≠stica, as m√©tricas de desempenho preditivo indicam baixa capacidade discriminat√≥ria. O modelo final apresentou acur√°cia de 14,5%, AUC de 26,7% e √≠ndice de Gini negativo (-46,6%), sugerindo que o modelo est√° classificando de forma inversa em rela√ß√£o √† realidade observada. O KS (Kolmogorov‚ÄìSmirnov) de 33,8% demonstra alguma separa√ß√£o entre bons e maus pagadores, por√©m ainda insuficiente para caracterizar um bom poder preditivo.")

st.write("Esses resultados indicam que, embora o modelo esteja estatisticamente ajustado e teoricamente coerente, sua performance preditiva √© insatisfat√≥ria, demandando ajustes adicionais ‚Äî como revis√£o de vari√°veis explicativas, novos tratamentos de outliers ou o uso de t√©cnicas n√£o lineares mais robustas (como √°rvores de decis√£o ou modelos de ensemble) para capturar padr√µes mais complexos nos dados.")
