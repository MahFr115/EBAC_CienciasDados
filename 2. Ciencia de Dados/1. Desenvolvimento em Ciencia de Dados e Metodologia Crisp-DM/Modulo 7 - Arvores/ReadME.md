# ğŸŒ³ MÃ³dulo 7 â€“ Ãrvores de DecisÃ£o (Parte 1)

Este mÃ³dulo introduz o primeiro algoritmo de **aprendizado de mÃ¡quina supervisionado** estudado no curso: as **Ãrvores de DecisÃ£o**, amplamente utilizadas pela sua facilidade de interpretaÃ§Ã£o e capacidade de solucionar problemas de **classificaÃ§Ã£o e regressÃ£o**. A partir deste ponto, entramos oficialmente no universo de modelagem preditiva em CiÃªncia de Dados.

---

## ğŸ¯ Objetivos do MÃ³dulo

âœ” Entender o funcionamento das Ãrvores de DecisÃ£o  
âœ” Diferenciar Ã¡rvores para **classificaÃ§Ã£o** e **regressÃ£o**  
âœ” Compreender a estrutura de uma Ã¡rvore: nÃ³ raiz, nÃ³s internos, ramos e folhas  
âœ” Calcular critÃ©rios de divisÃ£o e selecionar a melhor variÃ¡vel e ponto de corte  
âœ” Trabalhar com variÃ¡veis explicativas (X) e alvo (y)  
âœ” Ajustar modelos utilizando a biblioteca **Scikit-learn**  
âœ” Realizar tÃ©cnicas de **poda** para evitar overfitting  
âœ” Conhecer vantagens, limitaÃ§Ãµes e variaÃ§Ãµes como Random Forest e XGBoost  

---

## ğŸ§  Estrutura conceitual da Ã¡rvore

| Elemento | DescriÃ§Ã£o |
|---------|-----------|
| **NÃ³ raiz** | Primeiro ponto de decisÃ£o da Ã¡rvore |
| **NÃ³s internos** | Pontos de quebra ao longo do aprendizado |
| **Ramos** | Caminhos que conectam os nÃ³s |
| **Folhas** | Resultado final (classe ou valor predito) |
| **Profundidade** | NÃ­vel de camadas da Ã¡rvore |

---

## ğŸ“Š Processo de construÃ§Ã£o da Ã¡rvore

1. Carregar e preparar os dados  
2. Definir variÃ¡veis **explicativas (X)** e **alvo (y)**  
3. Avaliar possÃ­veis pontos de corte  
4. Escolher o critÃ©rio de divisÃ£o (ex: **Impureza de Gini**)  
5. Crescer a Ã¡rvore  
6. Aplicar **poda** (pruning) para reduzir complexidade  
7. Avaliar prÃ³s e contras  

---

## ğŸ“ CritÃ©rios e parÃ¢metros importantes

| Conceito | FunÃ§Ã£o |
|----------|--------|
| **Impureza de Gini** | Mede a pureza dos nÃ³s (quanto menor, melhor) |
| **C Alfa (Î±)** | ParÃ¢metro de complexidade usado na poda |
| **Ponto de corte** | Valor de divisÃ£o escolhido para separar dados |
| **Scikit-learn** | Biblioteca utilizada para implementaÃ§Ã£o |

---

##âœ‚ï¸ Poda (Pruning)

âœ” Evita overfitting  
âœ” Utiliza o parÃ¢metro `C alfa` e mÃ©todos como `'complex'`  
âœ” PÃ³s-poda: a Ã¡rvore cresce e depois Ã© reduzida ao ideal  

---

## âš–ï¸ Vantagens e desvantagens

| PrÃ³s | Contras |
|------|--------|
| Modelo de **caixa branca** (interpretÃ¡vel) | Pode sofrer **overfitting** |
| Funciona com variÃ¡veis categÃ³ricas e numÃ©ricas | DivisÃµes podem ficar enviesadas |
| Suporta mÃºltiplas saÃ­das | Pode exigir poda para eficiÃªncia |

---

## ğŸ“š GlossÃ¡rio

O arquivo **`Glossario.pdf`** contÃ©m todos os termos principais apresentados neste mÃ³dulo, como:  
`Ãrvore de ClassificaÃ§Ã£o`, `Ãrvore de RegressÃ£o`, `Gini`, `C Alfa`, `Random Forest`, `XGBoost`, `Profundidade`, `Folha`, `Ponto de Corte`, `Classificador (clf)`, entre outros.  
ğŸ“ *Local do arquivo: `./Glossario.pdf`*

---

## ğŸ›  Ferramentas Utilizadas

| Ferramenta | Finalidade |
|-----------|------------|
| **Scikit-learn (sklearn)** | ConstruÃ§Ã£o e poda da Ã¡rvore |
| **Pandas / NumPy** | EstruturaÃ§Ã£o dos dados |
| **Jupyter Notebook** | Treinamento interativo do modelo |

---

## ğŸ“Œ ImportÃ¢ncia deste mÃ³dulo

As Ãrvores de DecisÃ£o sÃ£o uma base essencial para compreender outras tÃ©cnicas de Machine Learning, incluindo algoritmos de conjunto como **Random Forest** e **XGBoost**, muito utilizados no mercado por sua alta performance. Dominar esse modelo Ã© o primeiro passo rumo ao aprendizado aprofundado de algoritmos supervisionados.
