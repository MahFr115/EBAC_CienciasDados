# ğŸ“‰ MÃ³dulo 26 â€“ PCA (AnÃ¡lise de Componentes Principais)

Este mÃ³dulo introduz a tÃ©cnica de **PCA (Principal Component Analysis)**, utilizada para **reduÃ§Ã£o de dimensionalidade** em conjuntos de dados com muitas variÃ¡veis. O objetivo Ã© transformar os dados em **novas componentes principais**, preservando o mÃ¡ximo possÃ­vel da variabilidade original.

O PCA se baseia em conceitos de Ãlgebra Linear â€” como **autovalores, autovetores, combinaÃ§Ãµes lineares e projeÃ§Ãµes** â€” estudados no **MÃ³dulo 25 (Ãlgebra Linear)**. A tÃ©cnica Ã© amplamente aplicada em etapas de prÃ©-processamento, visualizaÃ§Ã£o e otimizaÃ§Ã£o de modelos em Machine Learning.

---

## ğŸ¯ Objetivos do MÃ³dulo

âœ” Entender a reduÃ§Ã£o de dimensionalidade e quando aplicÃ¡-la  
âœ” Identificar a **maldiÃ§Ã£o da dimensionalidade** em datasets multivariados  
âœ” Criar **componentes principais** como novas variÃ¡veis projetadas  
âœ” Utilizar autovalores e autovetores para definir eixos de mÃ¡xima variÃ¢ncia  
âœ” Avaliar a **variÃ¢ncia explicada** por cada componente  
âœ” Aplicar critÃ©rios como **cotovelo** e **variÃ¢ncia mÃ­nima explicada** para seleÃ§Ã£o de componentes  
âœ” Implementar PCA com bibliotecas como `sklearn`  
âœ” Compreender impactos da reduÃ§Ã£o na performance de modelos  

---

## ğŸ“š ConteÃºdo Abordado

| Tema | DescriÃ§Ã£o |
|------|-----------|
| PCA (AnÃ¡lise de Componentes Principais) | TÃ©cnica de reduÃ§Ã£o de dimensionalidade |
| Dimensionalidade | NÃºmero de variÃ¡veis de um conjunto de dados |
| MaldiÃ§Ã£o da dimensionalidade | Perda de desempenho com muitas variÃ¡veis |
| Componentes principais | Novas variÃ¡veis formadas por combinaÃ§Ãµes lineares |
| RedundÃ¢ncia de informaÃ§Ã£o | CorrelaÃ§Ã£o alta entre variÃ¡veis originais |
| Variabilidade e variÃ¢ncia | DispersÃ£o dos dados e explicaÃ§Ã£o de variaÃ§Ã£o |
| Autovalores | Quantidade de variÃ¢ncia explicada por cada componente |
| Autovetores | DireÃ§Ãµes de maior variabilidade dos dados |
| VariÃ¢ncia explicada | Percentual de informaÃ§Ã£o retida por componente |
| CritÃ©rio de variÃ¢ncia explicada | DefiniÃ§Ã£o de threshold mÃ­nimo de retenÃ§Ã£o |
| CritÃ©rio do cotovelo | Escolha subjetiva com base em curva de variÃ¢ncia |
| Grid Search | OtimizaÃ§Ã£o de hiperparÃ¢metros (aplicÃ¡vel quando PCA Ã© parte de pipeline) |
| PCA com sklearn | ImplementaÃ§Ã£o prÃ¡tica em Python |

ğŸ“ GlossÃ¡rio completo disponÃ­vel em: **`Glossario.pdf`** :contentReference[oaicite:0]{index=0}

---

## ğŸ›  AplicaÃ§Ãµes na CiÃªncia de Dados

| AplicaÃ§Ã£o | Como o PCA contribui |
|-----------|---------------------|
| PrÃ©-processamento | Reduz ruÃ­do e redundÃ¢ncia |
| VisualizaÃ§Ã£o de dados | GeraÃ§Ã£o de grÃ¡ficos em 2D/3D a partir de dados multivariados |
| Performance de modelos | Menos variÃ¡veis â†’ menor complexidade |
| Combate ao overfitting | ReduÃ§Ã£o de variÃ¡veis irrelevantes |
| Pipeline com Scikit-Learn | Componente de transformaÃ§Ã£o integrado ao treinamento |

---

## ğŸ“Œ ImportÃ¢ncia do MÃ³dulo

O PCA Ã© uma ferramenta essencial para quem trabalha com **dados de alta dimensionalidade**, pois permite simplificar a estrutura do dataset sem comprometer (ou comprometendo minimamente) a quantidade de informaÃ§Ã£o. Dominar PCA Ã© importante nÃ£o apenas para modelagem, mas tambÃ©m para **compreensÃ£o da estrutura dos dados**.