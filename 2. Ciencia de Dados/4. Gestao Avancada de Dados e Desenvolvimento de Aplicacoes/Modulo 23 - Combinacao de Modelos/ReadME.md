# ğŸš€ MÃ³dulo 23 â€“ CombinaÃ§Ã£o de Modelos II

Dando continuidade ao estudo de **tÃ©cnicas de combinaÃ§Ã£o de modelos (ensemble learning)** iniciado no **MÃ³dulo 22 (Bagging & Random Forest)**, este mÃ³dulo apresenta o mÃ©todo de **Boosting**, uma estratÃ©gia poderosa que combina modelos fracos de forma sequencial para criar um preditor altamente robusto e preciso.

Ao contrÃ¡rio do Bagging, onde os modelos sÃ£o treinados de forma independente, o **Boosting constrÃ³i modelos de forma sequencial**, dando mais peso aos erros cometidos pelos modelos anteriores.

---

## ğŸ¯ Objetivos do MÃ³dulo

âœ” Entender os fundamentos do **Boosting**  
âœ” Comparar **Bagging x Boosting**  
âœ” Conhecer o funcionamento do **AdaBoost** (Adaptive Boosting)  
âœ” Aprender o conceito de **resÃ­duos como sinal de ajuste incremental**  
âœ” Compreender a ideia de **funÃ§Ã£o de perda e gradiente**  
âœ” Explorar o **Gradient Boosting Machine (GBM)**  
âœ” Conhecer o **Stochastic Gradient Boosting** e o uso de subamostragem  
âœ” Introduzir o **XGBoost**, otimizado com paralelismo e regularizaÃ§Ã£o  
âœ” Avaliar modelos com mÃ©tricas como **ROC AUC** e validaÃ§Ã£o fora do tempo  
âœ” Ajustar hiperparÃ¢metros e compreender critÃ©rios de parada  

---

## ğŸ“š ConteÃºdo Abordado

| Tema | DescriÃ§Ã£o |
|------|-----------|
| Boosting | Treinamento sequencial de modelos fracos |
| AdaBoost | Ajuste progressivo baseado na correÃ§Ã£o de erros |
| FunÃ§Ã£o de Perda | Mede o erro entre previsto e real |
| ResÃ­duos | Erros que guiam o aprendizado das prÃ³ximas Ã¡rvores |
| GBM (Gradient Boosting Machine) | Boosting guiado por gradiente da perda |
| Stochastic Gradient Boosting | Uso de subamostragem para melhorar generalizaÃ§Ã£o |
| XGBoost | ImplementaÃ§Ã£o otimizada com paralelismo, regularizaÃ§Ã£o e eficiÃªncia |
| Robustez | Capacidade de evitar overfitting |
| CritÃ©rio de Parada | NÃºmero de iteraÃ§Ãµes/Ã¡rvores ou convergÃªncia da perda |
| ValidaÃ§Ã£o Fora do Tempo | MÃ©todo de validaÃ§Ã£o temporal |
| ValidaÃ§Ã£o com troca | AlternÃ¢ncia de perÃ­odos para avaliar estabilidade |
| ROC AUC | MÃ©trica de separaÃ§Ã£o entre classes |
| Impacto relativo das variÃ¡veis | InterpretaÃ§Ã£o da importÃ¢ncia das features |

---

## ğŸ“‘ GlossÃ¡rio do MÃ³dulo

Os conceitos apresentados estÃ£o detalhados no arquivo:  
ğŸ“ **`Glossario.pdf`** :contentReference[oaicite:0]{index=0}

---

## ğŸ›  Ferramentas Utilizadas

| Ferramenta | Finalidade |
|-----------|-----------|
| **Scikit-Learn (AdaBoostClassifier, GradientBoostingClassifier)** | ConstruÃ§Ã£o de modelos Boosting |
| **XGBoost** | VersÃ£o otimizada do GBM |
| **Pandas / NumPy** | PreparaÃ§Ã£o de dados |
| **Matplotlib / Seaborn** | VisualizaÃ§Ã£o de mÃ©tricas como ROC |
| **GridSearchCV / RandomizedSearchCV** | Ajuste de hiperparÃ¢metros |

---

## ğŸ“Œ ImportÃ¢ncia do MÃ³dulo

O Boosting Ã© uma das tÃ©cnicas mais utilizadas em competiÃ§Ãµes e aplicaÃ§Ãµes reais por oferecer:

âœ… Alta performance preditiva  
âœ… CorreÃ§Ã£o iterativa de erros  
âœ… Maior capacidade de ajuste que uma Ã¡rvore individual  
âœ… Menor tendÃªncia a overfitting quando bem configurado  
âœ… Modelos de referÃªncia como XGBoost dominam desafios reais de Machine Learning  